{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b232ed",
   "metadata": {},
   "source": [
    "# 1. prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db6a3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09109dd4",
   "metadata": {},
   "source": [
    "## 1.1 download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c4d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: C:\\Users\\Lenovo\\.cache\\kagglehub\\datasets\\mnassrib\\jena-climate\\versions\\1\n",
      "Files moved to: c:\\Users\\Lenovo\\myFiles\\nauka\\studia\\sem7\\uczenie głębokie\\laby\\sequence_analysis\\data\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"../data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "path = kagglehub.dataset_download(\"mnassrib/jena-climate\")\n",
    "print(\"Downloaded to:\", path)\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    source = os.path.join(path, filename)\n",
    "    destination = os.path.join(OUTPUT_DIR, filename)\n",
    "    shutil.move(source, destination)\n",
    "\n",
    "print(f\"Files moved to: {os.path.abspath(OUTPUT_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90416de9",
   "metadata": {},
   "source": [
    "## 1.2 data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677912a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD\n",
      "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
      "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
      "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
      "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
      "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
      "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
      "\n",
      "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
      "0          3.33          3.11          0.22       1.94             3.12   \n",
      "1          3.23          3.02          0.21       1.89             3.03   \n",
      "2          3.21          3.01          0.20       1.88             3.02   \n",
      "3          3.26          3.07          0.19       1.92             3.08   \n",
      "4          3.27          3.08          0.19       1.92             3.09   \n",
      "\n",
      "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0       1307.75      1.03           1.75     152.3  \n",
      "1       1309.80      0.72           1.50     136.1  \n",
      "2       1310.24      0.19           0.63     171.6  \n",
      "3       1309.19      0.34           0.50     198.0  \n",
      "4       1309.00      0.32           0.63     214.3  \n",
      "INFO\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420551 entries, 0 to 420550\n",
      "Data columns (total 15 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Date Time        420551 non-null  object \n",
      " 1   p (mbar)         420551 non-null  float64\n",
      " 2   T (degC)         420551 non-null  float64\n",
      " 3   Tpot (K)         420551 non-null  float64\n",
      " 4   Tdew (degC)      420551 non-null  float64\n",
      " 5   rh (%)           420551 non-null  float64\n",
      " 6   VPmax (mbar)     420551 non-null  float64\n",
      " 7   VPact (mbar)     420551 non-null  float64\n",
      " 8   VPdef (mbar)     420551 non-null  float64\n",
      " 9   sh (g/kg)        420551 non-null  float64\n",
      " 10  H2OC (mmol/mol)  420551 non-null  float64\n",
      " 11  rho (g/m**3)     420551 non-null  float64\n",
      " 12  wv (m/s)         420551 non-null  float64\n",
      " 13  max. wv (m/s)    420551 non-null  float64\n",
      " 14  wd (deg)         420551 non-null  float64\n",
      "dtypes: float64(14), object(1)\n",
      "memory usage: 48.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"../data/jena_climate_2009_2016.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE, header=0)\n",
    "\n",
    "print(\"HEAD\")\n",
    "print(df.head())\n",
    "print(\"INFO\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330b5dc",
   "metadata": {},
   "source": [
    "## 1.3 drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9db0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"../data/filtered.csv\"\n",
    "\n",
    "df = df.drop(\"Tpot (K)\", axis=1)\n",
    "df.to_csv(OUTPUT_FILE, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4390f",
   "metadata": {},
   "source": [
    "## 1.4 divide into train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591cbad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 294385, 69.99983355169765%\n",
      "validation: 63083, 15.000083224151172%\n",
      "test: 63083, 15.000083224151172%\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = \"../data/filtered.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE, header=0)\n",
    "\n",
    "n = len(df)\n",
    "train_end = int(0.7 * n)\n",
    "val_end = int(0.85 * n)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]\n",
    "\n",
    "train_df.to_csv(\"../data/train_set.csv\", index=False, header=True)\n",
    "val_df.to_csv(\"../data/validation_set.csv\", index=False, header=True)\n",
    "test_df.to_csv(\"../data/test_set.csv\", index=False, header=True)\n",
    "\n",
    "print(f\"train: {len(train_df)}, {(len(train_df) / n) * 100}%\")\n",
    "print(f\"validation: {len(val_df)}, {(len(val_df) / n) * 100}%\")\n",
    "print(f\"test: {len(test_df)}, {(len(test_df) / n) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eee173",
   "metadata": {},
   "source": [
    "## 1.5 normalize data using min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342f0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLS = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\"\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[NUMERIC_COLS])\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train_set.csv\", header=0)\n",
    "val_df = pd.read_csv(\"../data/validation_set.csv\", header=0)\n",
    "test_df = pd.read_csv(\"../data/test_set.csv\", header=0)\n",
    "\n",
    "train_df[NUMERIC_COLS] = scaler.transform(train_df[NUMERIC_COLS])\n",
    "val_df[NUMERIC_COLS] = scaler.transform(val_df[NUMERIC_COLS])\n",
    "test_df[NUMERIC_COLS] = scaler.transform(test_df[NUMERIC_COLS])\n",
    "\n",
    "train_df.to_csv(\"../data/train_set_normalized.csv\", index=False, header=True)\n",
    "val_df.to_csv(\"../data/validation_set_normalized.csv\", index=False, header=True)\n",
    "test_df.to_csv(\"../data/test_set_normalized.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b37fc",
   "metadata": {},
   "source": [
    "## 1.6 create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19dd35",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m     17\u001b[0m forecast_horizon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 19\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_horizon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m create_sequences(val_data, sequence_length, forecast_horizon)\n\u001b[0;32m     21\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m create_sequences(test_data, sequence_length, forecast_horizon)\n",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, seq_length, forecast_horizon)\u001b[0m\n\u001b[0;32m     10\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m seq_length \u001b[38;5;241m-\u001b[39m forecast_horizon \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     13\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(data[i\u001b[38;5;241m+\u001b[39mseq_length:i\u001b[38;5;241m+\u001b[39mseq_length\u001b[38;5;241m+\u001b[39mforecast_horizon])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\myFiles\\nauka\\studia\\sem7\\uczenie głębokie\\laby\\sequence_analysis\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m \u001b[38;5;66;03m# Do we have a slicer (on rows)?\u001b[39;00m\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m-> 4096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) DataFrame?\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame):\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\myFiles\\nauka\\studia\\sem7\\uczenie głębokie\\laby\\sequence_analysis\\.venv\\lib\\site-packages\\pandas\\core\\generic.py:4371\u001b[0m, in \u001b[0;36mNDFrame._getitem_slice\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4366\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4367\u001b[0m \u001b[38;5;124;03m__getitem__ for the case where the key is a slice object.\u001b[39;00m\n\u001b[0;32m   4368\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4369\u001b[0m \u001b[38;5;66;03m# _convert_slice_indexer to determine if this slice is positional\u001b[39;00m\n\u001b[0;32m   4370\u001b[0m \u001b[38;5;66;03m#  or label based, and if the latter, convert to positional\u001b[39;00m\n\u001b[1;32m-> 4371\u001b[0m slobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_slice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slobj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4373\u001b[0m     \u001b[38;5;66;03m# reachable with DatetimeIndex\u001b[39;00m\n\u001b[0;32m   4374\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_indices_to_slice(\n\u001b[0;32m   4375\u001b[0m         slobj\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   4376\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\myFiles\\nauka\\studia\\sem7\\uczenie głębokie\\laby\\sequence_analysis\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:4229\u001b[0m, in \u001b[0;36mIndex._convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   4225\u001b[0m is_index_slice \u001b[38;5;241m=\u001b[39m is_valid_positional_slice(key)\n\u001b[0;32m   4227\u001b[0m \u001b[38;5;66;03m# TODO(GH#50617): once Series.__[gs]etitem__ is removed we should be able\u001b[39;00m\n\u001b[0;32m   4228\u001b[0m \u001b[38;5;66;03m#  to simplify this.\u001b[39;00m\n\u001b[1;32m-> 4229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   4230\u001b[0m     \u001b[38;5;66;03m# We always treat __getitem__ slicing as label-based\u001b[39;00m\n\u001b[0;32m   4231\u001b[0m     \u001b[38;5;66;03m# translate to locations\u001b[39;00m\n\u001b[0;32m   4232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetitem\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_index_slice \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m start \u001b[38;5;241m==\u001b[39m stop \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   4233\u001b[0m         \u001b[38;5;66;03m# exclude step=0 from the warning because it will raise anyway\u001b[39;00m\n\u001b[0;32m   4234\u001b[0m         \u001b[38;5;66;03m# start/stop both None e.g. [:] or [::-1] won't change.\u001b[39;00m\n\u001b[0;32m   4235\u001b[0m         \u001b[38;5;66;03m# exclude start==stop since it will be empty either way, or\u001b[39;00m\n\u001b[0;32m   4236\u001b[0m         \u001b[38;5;66;03m# will be [:] or [::-1] which won't change\u001b[39;00m\n\u001b[0;32m   4237\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   4238\u001b[0m             \u001b[38;5;66;03m# GH#49612\u001b[39;00m\n\u001b[0;32m   4239\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of obj[i:j] with a float-dtype index is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4244\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   4245\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"../data/train_set_normalized.csv\", header=0)\n",
    "val_data = pd.read_csv(\"../data/validation_set_normalized.csv\", header=0)\n",
    "test_data = pd.read_csv(\"../data/test_set_normalized.csv\", header=0)\n",
    "\n",
    "train_data = train_data[NUMERIC_COLS]\n",
    "val_data = val_data[NUMERIC_COLS]\n",
    "test_data = test_data[NUMERIC_COLS]\n",
    "\n",
    "def create_sequences(data, seq_length: int, forecast_horizon: int):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length:i+seq_length+forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 12 # based on last two hours\n",
    "forecast_horizon = 1 # predict next 10 minutes\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, sequence_length, forecast_horizon)\n",
    "X_val, y_val = create_sequences(val_data, sequence_length, forecast_horizon)\n",
    "X_test, y_test = create_sequences(test_data, sequence_length, forecast_horizon)\n",
    "\n",
    "np.save(\"../data/X_train.npy\", X_train)\n",
    "np.save(\"../data/y_train.npy\", y_train)\n",
    "\n",
    "np.save(\"../data/X_val.npy\", X_val)\n",
    "np.save(\"../data/y_val.npy\", y_val)\n",
    "\n",
    "np.save(\"../data/X_test.npy\", X_test)\n",
    "np.save(\"../data/y_test.npy\", y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
